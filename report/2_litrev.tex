\section{Literature Review}

Traditional approaches to multi-author writing style analysis mainly rely on lexical and syntactic feature based methods such as TF-IDF vectorization, character n-grams, and POS tag frequencies. These methods have demonstrated moderate performance in detecting style change at the paragraph level, but struggle with sentence-level changes due to limited contextual understanding. For instance, the PAN 2021 task utilized lexical similarity measures to detect authorship changes, achieving an F1 score of 0.73 on multi-author documents, indicating that feature-based methods were more effective for document-wide analysis but struggled with fine-grained author attribution \cite{paper6}. PAN 2022 introduced structural features such as indentation and average sentence length, further refining style change detection by incorporating discourse-level analysis \cite{paper4}. However, these methods were constrained by their reliance on handcrafted features and were susceptible to topic influence.

PAN 2023 and 2024 introduced more powerful techniques such as transformer based methods. The top-performing models in PAN 2023 leveraged pre-trained transformers like DeBERTaV3 and BERT for paragraph-level classification, achieving state-of-the-art F1 scores of 0.83 for the hardest dataset variant \cite{paper3}. These models incorporated contrastive learning and fine-tuning on task-specific datasets, allowing them to distinguish between subtle stylistic variations more effectively. However, their performance degraded when topic variations were minimized, highlighting the challenge of isolating style from content.

Another approach integrated contrastive learning with DeBERTaV3 and knowledge distillation for efficiency \cite{paper3}. This method allowed models to refine their representations of stylistic shifts while maintaining computational feasibility. Additionally, multi-stage fine-tuning techniques were introduced in PAN 2024, further improving performance by integrating both syntactic and semantic embeddings for style differentiation \cite{paper2}. Hybrid approaches were also explored, combining traditional and deep learning methods. One study used a stacking ensemble of lexical and neural models, achieving an F1 score of 0.82 on medium-difficulty datasets \cite{paper3}. Another team experimented with sentence-level style change detection using hierarchical clustering and transformer embeddings, demonstrating the potential for finer granularity in authorship attribution \cite{paper2}.

Performance evaluation has been standardized using the macro-averaged F1-score, with separate rankings for easy, medium, and hard datasets. PAN 2023 results highlighted the superiority of transformer-based models over traditional techniques, with the best-performing systems leveraging multi-stage fine-tuning, semantic similarity post-processing, and ensemble classifiers \cite{paper2}. However, performance variations across dataset difficulty levels suggest that improvements in data augmentation and domain adaptation are needed.

% Traditional approaches to Humour-aware Information Retrieval (HIR) rely on TF-IDF vectorization, and logistic regression models. These models demonstrate high precision for non-humorous text but struggle with puns, achieving F1 scores of 0.93 for non-puns but 0.73 for puns, indicating that the performance was better for identifying sentences without puns and moderate for sentences with puns \cite{rowmantomisalv2024}. They trained their models on a corpus containing a list of 61,268 pun and non-pun sentences. Another paper also utilizes the TF-IDF vectorization method for HIR, and were able to effectively identify and rank humorous texts within the corpus \cite{antoniadora2024}. More advanced techniques have also been applied, such as transformer-based models by Gepolava et al. \cite{gepolava2024}, who utilized the T5-transformer model for pun detection achieving an accuracy of 75.2\%. Their approach involved query expansion using synonyms, tokenization, and similarity scoring to capture the nuances of humour through LLMs. However, their model struggled with precision and recall, mainly due to data imbalance. Another notable approach integrated TF-IDF vectorization with logistic regression, and advanced neural translation models \cite{elagina2024}. Their model also showed promising results in preliminary testing, with high precision and recall, but faced challenges in official evaluations. Another team explored a variety of models including BM25, RM3, and BERT-based cross-encoders, with post filtering for humour detection \cite{ermakova2024}, trained on 61,268 documents of which 4,492 were humorous, while the other were non-humorous. Their results indicated that filtering on wordplay detection tasks improved performance significantly, though precision and recall remained low. 

% Dsilva and Bhardwaj's “PunDerstand @ CLEF JOKER 2024” \cite{dsilva2024punderstand} tackled humor classification using three distinct methods. Their guided annotation relied on a codebook to label humor types (e.g., irony, sarcasm) via structural cues like homophones or unexpected twists, offering a manual approach to pinpoint wordplay relevant to Task 1. Their multi-class classification with DeBERTa \cite{he2023debertav3}, fine-tuned on the JOKER 2023 corpus \cite{ermakova2023joker}, achieved a test accuracy of 0.6870 by leveraging disentangled attention mechanisms, which could score humor probability for Task 1 retrieval ranking. In contrast, their LLM prompting with GPT-4o \cite{achiam2023gpt4} yielded a lower accuracy of 0.4668, likely due to insufficient humor-specific fine-tuning, yet suggests potential for Task 2 if paired with structured guidelines to map humor categories across languages.

% Additional studies provide targeted advancements. Tang et al.'s “Pun-GAN” \cite{tang2019pungan} employed a generative adversarial network to translate puns, achieving a BLEU score of 0.45 for English-Chinese pairs by balancing semantic and phonetic fidelity, a technique adjustable for English-French translation in Task 2. Troiano et al. \cite{troiano2018exaggeration} used lexical analysis to detect exaggeration, identifying hyperbolic terms that could filter Task 1 retrieved texts for specific humor styles, though limited to single-type detection. Dsilva's thesis \cite{dsilva2024augmenting} embedded humor theory (e.g., incongruity) into LLM prompts, boosting context awareness that could refine Task 1 precision and Task 2 pun preservation, despite requiring computational overhead.

% Neural models like T5 and DeBERTa outperform TF-IDF's term-frequency focus, which misses pun-related context, yet face data imbalance issues. For Task 1, DeBERTa's attention mechanisms could enhance retrieval by scoring wordplay likelihood, while T5's tokenization struggles with recall. For Task 2, Pun-GAN's generative approach offers a starting point, but preserving phonetic humor in French remains unresolved. Integrating guided annotation with these models could bridge gaps, suggesting hybrid pipelines as a future focus.


% The task required systems to produce translations that maintained humor, even if the wordplay had to be adapted for the target language. This task, similar to JOKER 2025 Task 2, aims to advance the automatic translation and interpretation of puns across languages. Some of the approaches for this task are mentioned below:

% Task 3 of the JOKER-2023 challenge focused on the automatic translation of English puns into French and Spanish. It addresses the challenges involved in translating wordplay, which often relies on language-specific nuances \cite{ermakova2023joker3}. The goal was to preserve both the form and meaning of the original pun. The dataset included training data from translation contests, featuring English-to-French and English-to-Spanish translations, alongside a test set of distinct English puns to be translated into both target languages.

% A three-stage architecture based on T5 (SimpleT5) was used for translation from English to French and Spanish. The first two stages process the necessary information to concatenate the English sentence, which serves as input for the third neural network. To train the model, the dataset for Task 3 was augmented with data from Task 1. The DeepL translator was also used for comparison, revealing that DeepL produced better results \cite{ermakova2023joker3}. One of the runs for Task 3 of JOKER 2022 were also produced using DeepL \cite{galeano2022ljgg}.  

% The dataset used in the CLEF 2022 JOKER Task 3 for pun translation was primarily derived from the SemEval-2017 pun detection task, which included 3,387 annotated English-language puns. These puns were categorized into homographic and heterographic types based on their lexical properties. To create a parallel corpus for translation, a translation contest was organized, engaging both students and professional translators. The collected translations were further expanded by incorporating published literary translations and contributions from Master's students in translation studies. The final training dataset comprised 1,772 annotated English instances and 4,753 annotated French translations, structured in JSON format for participants. The test set contained 2,378 English instances, with target translations omitted for evaluation.

% Standard machine translation metrics like BLEU were deemed inadequate for evaluating pun translation due to the creative nature of wordplay. Instead, a manual evaluation was conducted by native French speakers, assessing factors such as lexical preservation, syntactic accuracy, semantic coherence, and wordplay retention. Additional error metrics included nonsense generation, syntax issues, lexical mismatches, and style shifts. Notably, humorousness shift and over-translation were also considered to determine whether the translated puns effectively retained the humor of the source language. These comprehensive evaluations highlighted the challenges faced by both human and automated translation systems in preserving the intended humor and linguistic creativity of puns.

% The translation of wordplay from English to Spanish was approached using BLOOMZ and mT5, which is an improved version of BLOOM \cite{preciado2023nlpalma}. 

% Several models, including SimpleT5, BLOOM, OpenAI, AI21, and those from the EasyNMT package (Opus-MT, mBART50-m2m, M2M-10), were tested for the English-Spanish translation task. OpenAI- and AI21-based models performed the best, while SimpleT5-based models ranked lowest. Despite these results, there is still significant room for improvement \cite{prnjak2023joker}. In JOKER's iteration of 2022 too, one of the teams participating, out of 4, used SimpleT5 library for the Google T5 (test-To-Text Transfer Trasnformer) model, which is based on the transfer learning with a unified text-to-text transformer \cite{raffel2020t5}.

% The approach used GPT-3, BLOOM, Opus-MT, mBART50-m2m models from EasyNMT, SimpleT5, and Google Translate for both English-Spanish and English-French translations. GPT-3 yielded the best results, while SimpleT5 produced incoherent sentences. GPT-3 and BLOOM achieved the highest scores on both datasets, though more data and time are needed for further improvement \cite{popova2023humor}.