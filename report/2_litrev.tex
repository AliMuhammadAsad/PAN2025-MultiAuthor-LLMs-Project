\section{Literature Review}

Traditional approaches to multi-author writing style analysis have relied on lexical and syntactic feature-based methods, including TF-IDF vectorization, character n-grams, and POS tag frequencies. These techniques have demonstrated moderate performance in detecting style changes at the paragraph level but struggle with sentence-level changes due to limited contextual understanding. One study leveraged a combination of lexical and structural features, such as indentation and sentence length, to refine style change detection by incorporating discourse-level analysis. However, handcrafted features were constrained by their susceptibility to topic influence, leading to inconsistent performance across datasets \cite{paper6} \cite{paper4}.

More recent approaches of 2024 and 2023 have shifted towards deep learning and transformer-based models to address these limitations. A paper in PAN 2024 explored bagging techniques, feature engineering, BERT-based classifiers and ensemble techniques combining different architectures such as BERT, RoBERTa, Electra and Llama2, using DetectGPT (Mistral-7B Falcon-7B) as their baseline models. They were able to achieve an F1-score of 0.924 as their best results on fine-tuned Mistral and Llama2 models \cite{paper1}. Another noteworthy paper of 2023 leveraged pre-trained transformers like DeBERTaV3 and BERT for paragraph-level classification, achieving state-of-the-art F1 scores of 0.83 for the hardest dataset variant \cite{paper3}. These models incorporated contrastive learning and fine-tuning on task-specific datasets, allowing them to distinguish between subtle stylistic variations more effectively. However, their performance degraded when topic variations were minimized, highlighting the challenge of isolating style from content. 

Another approach was used in 2023 where the authors performed data augmentation before training several models including RoBERTa, ELECTRA, and BERT. They achieved the best scores on RoBERTa with F1 scores of 0.996, 0.811, 0.814 for easy, medium, and hard datasets respectively \cite{paper_2023_1}. Another paper decided to use a different approach where they used a contrastive learning method to optimize the segment embedding output by the encoder of the pre-training model to obtain more similar vector spaces when processing sentences with similar styles. They called this CoSENT, and were able to get F1 scores of 0.915, 0.820, and 0.705 on easy, medium, and hard datasets respectively \cite{paper_2023_2}.

Tzu-Mi Lin et.al experimented with ensemble pre-trained transformer models including BERT, RoBERTa, and ALBERT. The best scores they got were for RoBERTa overall with F1 scores of 0.766, 0.503, and 0.705 for easy, medium, and hard datasets respectively \cite{paper_2022}. Hybrid Deep learning models have also been explored such as Bi-LSTM and BERT where BERT was used to create word embeddings which were then passed onto a BI-LSTM layer, followed by a CNN layer, then by a pooling layer, and finally through a fully connected layer \cite{paper_2022_2}. This model achieved F1 scores of 0.67, 0.40, and 0.65 for easy, medium, and hard datasets respectively.

% Traditional approaches to multi-author writing style analysis mainly rely on lexical and syntactic feature based methods such as TF-IDF vectorization, character n-grams, and POS tag frequencies. These methods have demonstrated moderate performance in detecting style change at the paragraph level, but struggle with sentence-level changes due to limited contextual understanding. For instance, the PAN 2021 task utilized lexical similarity measures to detect authorship changes, achieving an F1 score of 0.73 on multi-author documents, indicating that feature-based methods were more effective for document-wide analysis but struggled with fine-grained author attribution \cite{paper6}. PAN 2022 introduced structural features such as indentation and average sentence length, further refining style change detection by incorporating discourse-level analysis \cite{paper4}. However, these methods were constrained by their reliance on handcrafted features and were susceptible to topic influence.

% PAN 2023 and 2024 introduced more powerful techniques such as transformer based methods. The top-performing models in PAN 2023 leveraged pre-trained transformers like DeBERTaV3 and BERT for paragraph-level classification, achieving state-of-the-art F1 scores of 0.83 for the hardest dataset variant \cite{paper3}. These models incorporated contrastive learning and fine-tuning on task-specific datasets, allowing them to distinguish between subtle stylistic variations more effectively. However, their performance degraded when topic variations were minimized, highlighting the challenge of isolating style from content.

% Another approach integrated contrastive learning with DeBERTaV3 and knowledge distillation for efficiency \cite{paper3}. This method allowed models to refine their representations of stylistic shifts while maintaining computational feasibility. Additionally, multi-stage fine-tuning techniques were introduced in PAN 2024, further improving performance by integrating both syntactic and semantic embeddings for style differentiation \cite{paper2}. Hybrid approaches were also explored, combining traditional and deep learning methods. One study used a stacking ensemble of lexical and neural models, achieving an F1 score of 0.82 on medium-difficulty datasets \cite{paper3}. Another team experimented with sentence-level style change detection using hierarchical clustering and transformer embeddings, demonstrating the potential for finer granularity in authorship attribution \cite{paper2}.

% Performance evaluation has been standardized using the macro-averaged F1-score, with separate rankings for easy, medium, and hard datasets. PAN 2023 results highlighted the superiority of transformer-based models over traditional techniques, with the best-performing systems leveraging multi-stage fine-tuning, semantic similarity post-processing, and ensemble classifiers \cite{paper2}. However, performance variations across dataset difficulty levels suggest that improvements in data augmentation and domain adaptation are needed. Another paper in PAN 2024 explored bagging techniques, feature engineering, BERT-based classifiers and ensemble techniques combining different architectures such as BERT, RoBERTa, Electra and Llama2, using DetectGPT (Mistral-7B Falcon-7B) as their baseline models. They were able to achieve an F1-score of 0.924 as their best results on fine-tuned Mistral and Llama2 models \cite{paper1}.
