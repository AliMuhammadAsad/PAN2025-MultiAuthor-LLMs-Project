\section{Results}

This section presents the results of our experiments. The table below shows the performance of the models on the validation set. The models were evaluated based on the F1 score, which is the harmonic mean of precision and recall. The F1 score is a good metric for imbalanced datasets, as it considers both false positives and false negatives. The F1 score is presented for each level, easy, medium and hard.

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Level} & \textbf{F1 (Easy)} & \textbf{F1 (Medium)} & \textbf{F1 (Hard)} \\ \hline
    \textbf{Baseline (2024 Results)} & 0.414 & 0.506 & 0.495 \\ \hline
    \textbf{Simple BoW} & 0.653 & 0.650 & 0.573 \\ \hline
    \textbf{Improved BoW} & 0.791 & 0.666 & 0.613 \\ \hline
\end{tabular}
\caption{F1 scores of the models on the validation set}
\end{table}

The results show an improvement in the F1 score for the improved BoW model over the baseline and BoW model on all levels. We will next further improve upon our results by exploring ML models along with TF-IDF vectorization as they can better capture the semantics of the text, we will also train Deep learning models such as Bi-LSTM, and transformer based models such as DeBertaV3, RoBERTa, BERT, Mistral, and LLama models as they have been proved by literature to perform well in the previous iterations of this task. 